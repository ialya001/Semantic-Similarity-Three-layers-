{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec1c89c8",
   "metadata": {},
   "source": [
    "# 1.1 Locality Sensitive Hashing (LSH) Algorithm (First Layer)\n",
    "https://nbviewer.org/github/bassimeledath/quora_profile/blob/master/questions_analysis.ipynb\n",
    "\n",
    "https://www.pinecone.io/learn/locality-sensitive-hashing/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ad767dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "from datasketch import MinHash, MinHashLSHForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7d1a31d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess will split a string of text into individual tokens/shingles based on whitespace.\n",
    "def preprocess(text):\n",
    "    punc = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
    "    tokens = text\n",
    "    # Removing punctuations in string\n",
    "    # Using loop + punctuation string\n",
    "    for ele in tokens:\n",
    "        if ele in punc:\n",
    "            tokens = tokens.replace(ele, \"\")\n",
    "    tokens = text.lower()\n",
    "    tokens = tokens.split()\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e17ab52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of Permutations\n",
    "permutations = 128\n",
    "\n",
    "#Number of Recommendations to return\n",
    "num_recommendations = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "785e4aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_forest(data, perms):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    minhash = []\n",
    "    \n",
    "    for text in data['text']:\n",
    "        tokens = preprocess(text)\n",
    "        m = MinHash(num_perm=perms)\n",
    "        for s in tokens:\n",
    "            m.update(s.encode('utf8'))\n",
    "        minhash.append(m)\n",
    "        \n",
    "    forest = MinHashLSHForest(num_perm=perms)\n",
    "    \n",
    "    for i,m in enumerate(minhash):\n",
    "        forest.add(i,m)\n",
    "        \n",
    "    forest.index()\n",
    "    \n",
    "    print('It took %s seconds to build forest.' %(time.time()-start_time))\n",
    "    \n",
    "    return forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "eac018da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text, database, perms, num_results, forest):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    tokens = preprocess(text)\n",
    "    m = MinHash(num_perm=perms)\n",
    "    for s in tokens:\n",
    "        m.update(s.encode('utf8'))\n",
    "        \n",
    "    idx_array = np.array(forest.query(m, num_results))\n",
    "    if len(idx_array) == 0:\n",
    "        return None # if your query is empty, return none\n",
    "    result={}\n",
    "    result = database.iloc[idx_array]['ParagraphP']\n",
    "    \n",
    "    print('It took %s seconds to query forest.' %(time.time()-start_time))\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4093f9d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 0.023745059967041016 seconds to build forest.\n"
     ]
    }
   ],
   "source": [
    "db = pd.read_csv('/Users/ibrahim/Desktop/paragraphP.csv')#We have 67 Sentences\n",
    "db['text'] = db['ParagraphP']\n",
    "forest = get_forest(db, permutations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "91f54c65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 0.007851839065551758 seconds to query forest.\n",
      "\n",
      " Top similar sentences \n",
      " 0     have heard, O fortunate king, that a wealthy ...\n",
      "1    When he had finished, the ‘ifrit said: ‘Stop t...\n",
      "2    He took his seat by the merchant’s side and pr...\n",
      "Name: ParagraphP, dtype: object\n"
     ]
    }
   ],
   "source": [
    "num_recommendations = 10\n",
    "query = \"t is related  O auspicious King that there was a merchant of the merchants who had much wealth and business in various cities Now on a day he mounted horse and went forth to re cover monies in certain towns and the heat sore oppressed him so he sat beneath a tree and putting his hand into his saddle bags took thence some broken bread and dry dates and began to break his fast When he had ended eating the dates he threw away the stones with force and lo an Ifrit appeared huge of stature and brandishing a drawn sword wherewith he approached the mer chant and said Stand up that I may slay thee even as thou slewest my son Asked the merchant How have I slain thy son and he answered When thou atest dates and threwest away the stones they struck my son full in the breast as he was walking by so that he died forthwith F40 Quoth the merchant Verily from Allah we proceeded and unto Allah are we re turning There is no Majesty and there is no Might save in Allah the Glorious the Great If I slew thy son I slew him by chance medley\".lower()\n",
    "result = predict(query, db, permutations, num_recommendations, forest)\n",
    "print('\\n Top similar sentences \\n', result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "91f0712d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' have heard, O fortunate king, that a wealthy merchant, who had many dealings throughout the lands, rode out one day to settle a matter of business in one of them. When it became hot, he sat down under a tree and put his hand in his saddlebag, from which he took out a piece of bread and a date. He ate and when he had finished with the date he threw away its stone, at which a huge ‘ifrit appeared, with a drawn sword in his hand. This ‘ifrit came up to the merchant and said: ‘Get up so that I can kill you as you killed my son.’ ‘How did I kill your son?’ asked the merchant, and the ‘ifrit told him: ‘When you ate that date and threw away the stone, it struck my son in the chest as he was walking, and he died instantly.’ ‘We belong to God and to Him do we return,’ recited the merchant, adding: ‘There is no might and no power except with God, the Exalted, the Omnipotent. If I killed him, this was by accident, so please forgive me.’ ‘I must kill you,’ insisted the ‘ifrit, and he dragged off the merchant, threw him down on the ground and raised his sword to strike.\\n\\nWith tears in his eyes, the merchant exclaimed: ‘I entrust my affair to God!’ and he then recited these lines:'\n",
      " 'When he had finished, the ‘ifrit said: ‘Stop talking, for, by God, I am most certainly going to kill you.’ ‘‘Ifrit,’ the merchant said, ‘I am a wealthy man, with a wife and children; I have debts and I hold deposits, so let me go home and give everyone their due before returning to you at the start of the new year. I shall take a solemn oath and swear by God that I shall come back to you and you can then do what you want with me. God will be the guarantor of this.’ The ‘ifrit trusted him and let him go, after which he went home, settled all his affairs, and gave everyone what was owed them. He told his wife and children what had happened, gave them his injunctions and stayed with them until the end of the year, when he got up, performed the ritual ablution and, with his shroud under his arm, said goodbye to his family and all his relations as well as his neighbours, and set off reluctantly, while they all wept and wailed. He came to the orchard on what was New Year’s Day, and as he sat there weeping over his fate, a very old man approached him, leading a gazelle on a chain. The newcomer greeted him and asked him why he was sitting there alone, when the place was a haunt of jinn. The merchant told the story of his encounter with the ‘ifrit, and the old man exclaimed: ‘By God, brother, you are a very pious man and your story is so wonderful that were it written with needles on the corners of men’s eyes, it would be a lesson for those who take heed.’'\n",
      " 'He took his seat by the merchant’s side and promised not to leave until he had seen what happened to him with the ‘ifrit. As the two of them sat there talking, the merchant was overcome by an access of fear together with ever-increasing distress and apprehension. It was at this point that a second old man arrived, having with him two black Salukis. After greeting the two men, he asked them why they were sitting in this haunt of jinn and they told him the story from beginning to end. No sooner had he sat down with them than a third old man, with a dappled mule, came up, greeted them and asked why they were there, at which they repeated the whole story – but there is no point in going over it again.']\n"
     ]
    }
   ],
   "source": [
    "#Create a list of candiates to be taken to the next layer which is Cosine Simialrity\n",
    "candidates=[]\n",
    "candidates=result.values[:]\n",
    "print(candidates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84345d64",
   "metadata": {},
   "source": [
    "# 1.2 Cosine Similarity (The second layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b8a608",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/how-to-rank-text-content-by-semantic-similarity-4d2419a84c32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "8bf0402f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from re import sub\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "#query_string = 'fruit and vegetables'\n",
    "#documents = ['cars drive on the road', 'tomatoes are actually fruit']\n",
    "\n",
    "stopwords = []\n",
    "\n",
    "# From: https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/soft_cosine_tutorial.ipynb\n",
    "def preprocess(doc):\n",
    "    # Tokenize, clean up input document string\n",
    "    doc = sub(r'<img[^<>]+(>|$)', \" image_token \", doc)\n",
    "    doc = sub(r'<[^<>]+(>|$)', \" \", doc)\n",
    "    doc = sub(r'\\[img_assist[^]]*?\\]', \" \", doc)\n",
    "    doc = sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', \" url_token \", doc)\n",
    "    return [token for token in simple_preprocess(doc, min_len=0, max_len=float(\"inf\")) if token not in stopwords]\n",
    "\n",
    "# Preprocess the documents, including the query string\n",
    "corpus = [preprocess(document) for document in candidates]\n",
    "query = preprocess(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "e15a12e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 329/329 [00:02<00:00, 126.75it/s]\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.similarities import WordEmbeddingSimilarityIndex\n",
    "from gensim.similarities import SparseTermSimilarityMatrix\n",
    "from gensim.similarities import SoftCosineSimilarity\n",
    "\n",
    "# Load the model: this is a big file, can take a while to download and open\n",
    "glove = api.load(\"glove-wiki-gigaword-50\")    \n",
    "similarity_index = WordEmbeddingSimilarityIndex(glove)\n",
    "\n",
    "# Build the term dictionary, TF-idf model\n",
    "dictionary = Dictionary(corpus+[query])\n",
    "tfidf = TfidfModel(dictionary=dictionary)\n",
    "\n",
    "# Create the term similarity matrix.  \n",
    "similarity_matrix = SparseTermSimilarityMatrix(similarity_index, dictionary, tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "0f6386d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \t 0.785 \t  have heard, O fortunate king, that a wealthy merchant, who had many dealings throughout the lands, rode out one day to settle a matter of business in one of them. When it became hot, he sat down under a tree and put his hand in his saddlebag, from which he took out a piece of bread and a date. He ate and when he had finished with the date he threw away its stone, at which a huge ‘ifrit appeared, with a drawn sword in his hand. This ‘ifrit came up to the merchant and said: ‘Get up so that I can kill you as you killed my son.’ ‘How did I kill your son?’ asked the merchant, and the ‘ifrit told him: ‘When you ate that date and threw away the stone, it struck my son in the chest as he was walking, and he died instantly.’ ‘We belong to God and to Him do we return,’ recited the merchant, adding: ‘There is no might and no power except with God, the Exalted, the Omnipotent. If I killed him, this was by accident, so please forgive me.’ ‘I must kill you,’ insisted the ‘ifrit, and he dragged off the merchant, threw him down on the ground and raised his sword to strike.\n",
      "\n",
      "With tears in his eyes, the merchant exclaimed: ‘I entrust my affair to God!’ and he then recited these lines:\n"
     ]
    }
   ],
   "source": [
    "# Compute Soft Cosine Measure between the query and the documents.\n",
    "# From: https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/soft_cosine_tutorial.ipynb\n",
    "query_tf = tfidf[dictionary.doc2bow(query)]\n",
    "\n",
    "index = SoftCosineSimilarity(\n",
    "            tfidf[[dictionary.doc2bow(document) for document in corpus]],\n",
    "            similarity_matrix)\n",
    "\n",
    "doc_similarity_scores = index[query_tf] \n",
    "\n",
    "\n",
    "# Output the sorted similarity scores and documents\n",
    "NW_Candiadates=[]\n",
    "sorted_indexes = np.argsort(doc_similarity_scores)[::-1]\n",
    "for idx in sorted_indexes:\n",
    "    if doc_similarity_scores[idx]>0.700:\n",
    "        print(f'{idx} \\t {doc_similarity_scores[idx]:0.3f} \\t {candidates[idx]}')\n",
    "        NW_Candiadates.append(candidates[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "9d6ba7db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' have heard, O fortunate king, that a wealthy merchant, who had many dealings throughout the lands, rode out one day to settle a matter of business in one of them. When it became hot, he sat down under a tree and put his hand in his saddlebag, from which he took out a piece of bread and a date. He ate and when he had finished with the date he threw away its stone, at which a huge ‘ifrit appeared, with a drawn sword in his hand. This ‘ifrit came up to the merchant and said: ‘Get up so that I can kill you as you killed my son.’ ‘How did I kill your son?’ asked the merchant, and the ‘ifrit told him: ‘When you ate that date and threw away the stone, it struck my son in the chest as he was walking, and he died instantly.’ ‘We belong to God and to Him do we return,’ recited the merchant, adding: ‘There is no might and no power except with God, the Exalted, the Omnipotent. If I killed him, this was by accident, so please forgive me.’ ‘I must kill you,’ insisted the ‘ifrit, and he dragged off the merchant, threw him down on the ground and raised his sword to strike.\\n\\nWith tears in his eyes, the merchant exclaimed: ‘I entrust my affair to God!’ and he then recited these lines:']\n"
     ]
    }
   ],
   "source": [
    "print(NW_Candiadates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca4b69f",
   "metadata": {},
   "source": [
    "# 1.3 Needleman Wunch (The Last Layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "95196d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "  ['t', 'is', 'related', 'o', 'auspicious', 'king', 'that', 'there', 'was', 'a', 'merchant', 'of', 'the', 'merchants', 'who', 'had', 'much', 'wealth', 'and', 'business', 'in', 'various', 'cities', 'now', 'on', 'a', 'day', 'he', 'mounted', 'horse', 'and', 'went', 'forth', 'to', 're', 'cover', 'monies', 'in', 'certain', 'towns', 'and', 'the', 'heat', 'sore', 'oppressed', 'him', 'so', 'he', 'sat', 'beneath', 'a', 'tree', 'and', 'putting', 'his', 'hand', 'into', 'his', 'saddle', 'bags', 'took', 'thence', 'some', 'broken', 'bread', 'and', 'dry', 'dates', 'and', 'began', 'to', 'break', 'his', 'fast', 'when', 'he', 'had', 'ended', 'eating', 'the', 'dates', 'he', 'threw', 'away', 'the', 'stones', 'with', 'force', 'and', 'lo', 'an', 'ifrit', 'appeared', 'huge', 'of', 'stature', 'and', 'brandishing', 'a', 'drawn', 'sword', 'wherewith', 'he', 'approached', 'the', 'mer', 'chant', 'and', 'said', 'stand', 'up', 'that', 'i', 'may', 'slay', 'thee', 'even', 'as', 'thou', 'slewest', 'my', 'son', 'asked', 'the', 'merchant', 'how', 'have', 'i', 'slain', 'thy', 'son', 'and', 'he', 'answered', 'when', 'thou', 'atest', 'dates', 'and', 'threwest', 'away', 'the', 'stones', 'they', 'struck', 'my', 'son', 'full', 'in', 'the', 'breast', 'as', 'he', 'was', 'walking', 'by', 'so', 'that', 'he', 'died', 'forthwith', 'f', 'quoth', 'the', 'merchant', 'verily', 'from', 'allah', 'we', 'proceeded', 'and', 'unto', 'allah', 'are', 'we', 're', 'turning', 'there', 'is', 'no', 'majesty', 'and', 'there', 'is', 'no', 'might', 'save', 'in', 'allah', 'the', 'glorious', 'the', 'great', 'if', 'i', 'slew', 'thy', 'son', 'i', 'slew', 'him', 'by', 'chance', 'medley'] \n",
      "\n",
      "Target Sentence: \n",
      "  have heard, O fortunate king, that a wealthy merchant, who had many dealings throughout the lands, rode out one day to settle a matter of business in one of them. When it became hot, he sat down under a tree and put his hand in his saddlebag, from which he took out a piece of bread and a date. He ate and when he had finished with the date he threw away its stone, at which a huge ‘ifrit appeared, with a drawn sword in his hand. This ‘ifrit came up to the merchant and said: ‘Get up so that I can kill you as you killed my son.’ ‘How did I kill your son?’ asked the merchant, and the ‘ifrit told him: ‘When you ate that date and threw away the stone, it struck my son in the chest as he was walking, and he died instantly.’ ‘We belong to God and to Him do we return,’ recited the merchant, adding: ‘There is no might and no power except with God, the Exalted, the Omnipotent. If I killed him, this was by accident, so please forgive me.’ ‘I must kill you,’ insisted the ‘ifrit, and he dragged off the merchant, threw him down on the ground and raised his sword to strike.\n",
      "\n",
      "With tears in his eyes, the merchant exclaimed: ‘I entrust my affair to God!’ and he then recited these lines: \n",
      "\n",
      "Perecent Identity using NW: 1.21\n"
     ]
    }
   ],
   "source": [
    "from minineedle import needle, core\n",
    "\n",
    "#query= 'it hath reached me, o auspicious king, that there was a fisher man well stricken in years who had a wife and three children, and withal was of poor condition.'\n",
    "\n",
    "threshold = 10.0\n",
    "R = {} # Dicitonary to Save query, target sentence, precent identity and the actual aligmnet\n",
    "\n",
    "for i in NW_Candiadates:\n",
    "    alignment = needle.NeedlemanWunsch(query, i)\n",
    "    x = alignment.get_identity()\n",
    "    y = alignment \n",
    "    R[i]=[x , query, i, y]\n",
    "max_value = max(R.values())\n",
    "print('Query:\\n ',max_value[1],'\\n')\n",
    "print('Target Sentence: \\n',max_value[2],'\\n')\n",
    "print('Perecent Identity using NW:',max_value[0])\n",
    "#print('\\n',max_value[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452e843b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64523c0d",
   "metadata": {},
   "source": [
    "# First Filter (Cosine Similarity)\n",
    "https://www.machinelearningplus.com/nlp/cosine-similarity/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a9fb6e86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>after</th>\n",
       "      <th>all</th>\n",
       "      <th>almighty</th>\n",
       "      <th>an</th>\n",
       "      <th>and</th>\n",
       "      <th>another</th>\n",
       "      <th>are</th>\n",
       "      <th>asaf</th>\n",
       "      <th>asked</th>\n",
       "      <th>back</th>\n",
       "      <th>...</th>\n",
       "      <th>which</th>\n",
       "      <th>who</th>\n",
       "      <th>whom</th>\n",
       "      <th>wife</th>\n",
       "      <th>wise</th>\n",
       "      <th>with</th>\n",
       "      <th>worked</th>\n",
       "      <th>would</th>\n",
       "      <th>you</th>\n",
       "      <th>yourself</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>this, however, is not more surprising than the tale of the fisherman.’ when the king asked what that was, she went on:\\n\\ni have heard, o fortunate king, that there once was a poor, elderly fisherman with a wife and three children, who was in the habit of casting his net exactly four times each day.</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>he took one end of it to the shore and fixed it to a peg that he drove in there, after which he stripped and dived into the sea beside it, where he continued tugging until he managed to get it up.</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>he climbed out delightedly, put his clothes back on and went up to the net, only to find that what was in it was a dead donkey, and that the donkey had made a hole in the net.</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>then, when he opened it up, he found in it a brass bottle with a lead seal, imprinted with the inscription of our master solomon, the son of david, on both of whom be peace.</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i’ll open it up and have a look before selling it.’ he took out a knife and worked on the lead until he had removed it from the bottle, which he then put down on the ground, shaking it in order to pour out its contents.</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>when it had all come out, it collected and solidified; a tremor ran through it and it became an ‘ifrit with his head in the clouds and his feet on the earth.</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>he encouraged himself, saying that almighty god would show favour and reciting:\\n\\nwhen you are faced with hardship, clothe yourself\\n\\nin noble patience; that is more resolute.</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>invoking the name of god, he made another cast, waited until the net had settled, and found it heavier and more difficult to move than before.</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>solomon sent his vizier, asaf, to fetch me to him under duress, and i was forced to go with him in a state of humiliation to stand before solomon.</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>how many wise men lie hidden in the earth!</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 174 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    after  all  almighty  an  \\\n",
       "this, however, is not more surprising than the ...      0    0         0   0   \n",
       "he took one end of it to the shore and fixed it...      1    0         0   0   \n",
       "he climbed out delightedly, put his clothes bac...      0    0         0   0   \n",
       "then, when he opened it up, he found in it a br...      0    0         0   0   \n",
       "i’ll open it up and have a look before selling ...      0    0         0   0   \n",
       "when it had all come out, it collected and soli...      0    1         0   1   \n",
       "he encouraged himself, saying that almighty god...      0    0         1   0   \n",
       "invoking the name of god, he made another cast,...      0    0         0   0   \n",
       "solomon sent his vizier, asaf, to fetch me to h...      0    0         0   0   \n",
       "how many wise men lie hidden in the earth!              0    0         0   0   \n",
       "\n",
       "                                                    and  another  are  asaf  \\\n",
       "this, however, is not more surprising than the ...    1        0    0     0   \n",
       "he took one end of it to the shore and fixed it...    2        0    0     0   \n",
       "he climbed out delightedly, put his clothes bac...    2        0    0     0   \n",
       "then, when he opened it up, he found in it a br...    0        0    0     0   \n",
       "i’ll open it up and have a look before selling ...    2        0    0     0   \n",
       "when it had all come out, it collected and soli...    3        0    0     0   \n",
       "he encouraged himself, saying that almighty god...    1        0    1     0   \n",
       "invoking the name of god, he made another cast,...    2        1    0     0   \n",
       "solomon sent his vizier, asaf, to fetch me to h...    1        0    0     1   \n",
       "how many wise men lie hidden in the earth!            0        0    0     0   \n",
       "\n",
       "                                                    asked  back  ...  which  \\\n",
       "this, however, is not more surprising than the ...      1     0  ...      0   \n",
       "he took one end of it to the shore and fixed it...      0     0  ...      1   \n",
       "he climbed out delightedly, put his clothes bac...      0     1  ...      0   \n",
       "then, when he opened it up, he found in it a br...      0     0  ...      0   \n",
       "i’ll open it up and have a look before selling ...      0     0  ...      1   \n",
       "when it had all come out, it collected and soli...      0     0  ...      0   \n",
       "he encouraged himself, saying that almighty god...      0     0  ...      0   \n",
       "invoking the name of god, he made another cast,...      0     0  ...      0   \n",
       "solomon sent his vizier, asaf, to fetch me to h...      0     0  ...      0   \n",
       "how many wise men lie hidden in the earth!              0     0  ...      0   \n",
       "\n",
       "                                                    who  whom  wife  wise  \\\n",
       "this, however, is not more surprising than the ...    1     0     1     0   \n",
       "he took one end of it to the shore and fixed it...    0     0     0     0   \n",
       "he climbed out delightedly, put his clothes bac...    0     0     0     0   \n",
       "then, when he opened it up, he found in it a br...    0     1     0     0   \n",
       "i’ll open it up and have a look before selling ...    0     0     0     0   \n",
       "when it had all come out, it collected and soli...    0     0     0     0   \n",
       "he encouraged himself, saying that almighty god...    0     0     0     0   \n",
       "invoking the name of god, he made another cast,...    0     0     0     0   \n",
       "solomon sent his vizier, asaf, to fetch me to h...    0     0     0     0   \n",
       "how many wise men lie hidden in the earth!            0     0     0     1   \n",
       "\n",
       "                                                    with  worked  would  you  \\\n",
       "this, however, is not more surprising than the ...     1       0      0    0   \n",
       "he took one end of it to the shore and fixed it...     0       0      0    0   \n",
       "he climbed out delightedly, put his clothes bac...     0       0      0    0   \n",
       "then, when he opened it up, he found in it a br...     2       0      0    0   \n",
       "i’ll open it up and have a look before selling ...     0       1      0    0   \n",
       "when it had all come out, it collected and soli...     1       0      0    0   \n",
       "he encouraged himself, saying that almighty god...     1       0      1    1   \n",
       "invoking the name of god, he made another cast,...     0       0      0    0   \n",
       "solomon sent his vizier, asaf, to fetch me to h...     1       0      0    0   \n",
       "how many wise men lie hidden in the earth!             0       0      0    0   \n",
       "\n",
       "                                                    yourself  \n",
       "this, however, is not more surprising than the ...         0  \n",
       "he took one end of it to the shore and fixed it...         0  \n",
       "he climbed out delightedly, put his clothes bac...         0  \n",
       "then, when he opened it up, he found in it a br...         0  \n",
       "i’ll open it up and have a look before selling ...         0  \n",
       "when it had all come out, it collected and soli...         0  \n",
       "he encouraged himself, saying that almighty god...         1  \n",
       "invoking the name of god, he made another cast,...         0  \n",
       "solomon sent his vizier, asaf, to fetch me to h...         0  \n",
       "how many wise men lie hidden in the earth!                 0  \n",
       "\n",
       "[10 rows x 174 columns]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Create the Document Term Matrix\n",
    "count_vectorizer = CountVectorizer(stop_words='english')\n",
    "count_vectorizer = CountVectorizer()\n",
    "sparse_matrix = count_vectorizer.fit_transform(candidates)\n",
    "\n",
    "# OPTIONAL: Convert Sparse Matrix to Pandas Dataframe if you want to see the word frequencies.\n",
    "doc_term_matrix = sparse_matrix.todense()\n",
    "df = pd.DataFrame(doc_term_matrix, \n",
    "                  columns=count_vectorizer.get_feature_names(), \n",
    "                  index=[candidates])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5eeb78a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.19885368 0.48418203 0.3138032  0.23692416 0.26440136\n",
      "  0.21025857 0.31008684 0.15540357 0.18871284]\n",
      " [0.19885368 1.         0.44003599 0.4322906  0.62523197 0.40708658\n",
      "  0.20382711 0.44088265 0.26782228 0.10976426]\n",
      " [0.48418203 0.44003599 1.         0.27288841 0.48156157 0.42217828\n",
      "  0.22333125 0.43915503 0.30567718 0.2227177 ]\n",
      " [0.3138032  0.4322906  0.27288841 1.         0.45782444 0.34061366\n",
      "  0.16251869 0.31957418 0.17795362 0.14586499]\n",
      " [0.23692416 0.62523197 0.48156157 0.45782444 1.         0.5429066\n",
      "  0.12951987 0.44570018 0.14182079 0.15499685]\n",
      " [0.26440136 0.40708658 0.42217828 0.34061366 0.5429066  1.\n",
      "  0.1530433  0.3761774  0.14663103 0.18314742]\n",
      " [0.21025857 0.20382711 0.22333125 0.16251869 0.12951987 0.1530433\n",
      "  1.         0.16951588 0.08495482 0.06189845]\n",
      " [0.31008684 0.44088265 0.43915503 0.31957418 0.44570018 0.3761774\n",
      "  0.16951588 1.         0.22273842 0.12171612]\n",
      " [0.15540357 0.26782228 0.30567718 0.17795362 0.14182079 0.14663103\n",
      "  0.08495482 0.22273842 1.         0.05083286]\n",
      " [0.18871284 0.10976426 0.2227177  0.14586499 0.15499685 0.18314742\n",
      "  0.06189845 0.12171612 0.05083286 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Compute Cosine Similarity\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "print(cosine_similarity(df, df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c43779eb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Shape of passed values is (10, 174), indices imply (1, 174)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/82/58plxq9d79373wycjp789qfm0000gn/T/ipykernel_5345/1406421980.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mquery_term_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msparse_matrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtodense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m df_query = pd.DataFrame(doc_term_matrix, \n\u001b[0m\u001b[1;32m      3\u001b[0m                   \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcount_vectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                   index=[query])\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    670\u001b[0m                 )\n\u001b[1;32m    671\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 672\u001b[0;31m                 mgr = ndarray_to_mgr(\n\u001b[0m\u001b[1;32m    673\u001b[0m                     \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m                     \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36mndarray_to_mgr\u001b[0;34m(values, index, columns, dtype, copy, typ)\u001b[0m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m     \u001b[0m_check_values_indices_shape_match\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"array\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36m_check_values_indices_shape_match\u001b[0;34m(values, index, columns)\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[0mpassed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m         \u001b[0mimplied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 393\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Shape of passed values is {passed}, indices imply {implied}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Shape of passed values is (10, 174), indices imply (1, 174)"
     ]
    }
   ],
   "source": [
    "query_term_matrix = sparse_matrix.todense()\n",
    "df_query = pd.DataFrame(doc_term_matrix, \n",
    "                  columns=count_vectorizer.get_feature_names(), \n",
    "                  index=[query])\n",
    "\n",
    "df_query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7345b8",
   "metadata": {},
   "source": [
    "# NW to test our candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "35abcb2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "  it hath reached me, o auspicious king, that there was a fisher man well stricken in years who had a wife and three children, and withal was of poor condition. \n",
      "\n",
      "Target Sentence: \n",
      " he climbed out delightedly, put his clothes back on and went up to the net, only to find that what was in it was a dead donkey, and that the donkey had made a hole in the net. \n",
      "\n",
      "Perecent Identity using NW: 37.19\n",
      "\n",
      " Alignment of Query and Target Sentence is:\n",
      "\t-----i---t hath re-ach-ed me, --o auspicio--us ---k-in--g, ---t--hat- the-re-- -was -a fisher -man well --stricken in years who -had a wife-- and three children, and withal wa-s of poor condit-ion--.\n",
      "\the climbed out- delighted-ly, put his- clothes back on and went up to the net, only to fi--nd that what was- i---n it -wa-s --a dead -donkey, and th-at the d-on--key ---had made -a hole -in- the net.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from minineedle import needle, core\n",
    "\n",
    "query= 'it hath reached me, o auspicious king, that there was a fisher man well stricken in years who had a wife and three children, and withal was of poor condition.'\n",
    "\n",
    "threshold = 10.0\n",
    "R = {} # Dicitonary to Save query, target sentence, precent identity and the actula aligmnet\n",
    "\n",
    "for i in candidates:\n",
    "    alignment = needle.NeedlemanWunsch(query.lower(), i.lower())\n",
    "    x = alignment.get_identity()\n",
    "    y = alignment \n",
    "    R[i]=[x , query, i, y]\n",
    "max_value = max(R.values())\n",
    "print('Query:\\n ',max_value[1],'\\n')\n",
    "print('Target Sentence: \\n',max_value[2],'\\n')\n",
    "print('Perecent Identity using NW:',max_value[0])\n",
    "print('\\n',max_value[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3ec480",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e063cc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "27c7c18e",
   "metadata": {},
   "source": [
    "# LSH Algorithm 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "2f90d7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess will split a string of text into individual tokens/shingles based on whitespace.\n",
    "def preprocess(text):\n",
    "    tokens = text.lower()\n",
    "    tokens = tokens.split()\n",
    "    return tokens\n",
    "\n",
    "a = 'It is related, O auspicious King, that there was a merchant of the merchants who had much wealth, and business in various cities'\n",
    "b = 'I have heard, O fortunate king, that a wealthy merchant, who had many dealings throughout the lands, rode out one day to settle a matter of business in one of them.'\n",
    "c = \"When it became hot, he sat down under a tree and put his hand in his saddlebag, from which he took out a piece of bread and a date\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "a254af3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Steps 2-7\n",
    "def shingle(text: str, k: int):\n",
    "    shingle_set = []\n",
    "    for i in range(len(text) - k+1):\n",
    "        shingle_set.append(text[i:i+k])\n",
    "    return set(shingle_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "8d20c6b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ed', 'ch', 'th', 'au', 'nt', ' h', 'ea', 'ic', 'ng', 'g,', 'h,', 'sp', 'Ki', 'va', 'O ', ' m', 'ts', ' w', 'me', 'ho', 'ha', 'he', 'te', 'ie', ' a', 't ', 'ri', 'mu', 'we', 'f ', 'of', 'la', ' b', 'h ', 'ti', ' t', 'ci', 'si', 'es', 'bu', 'an', 'is', 'ar', 'uc', 'It', 'o ', 'e ', 'pi', 'el', ', ', 're', 'as', ' O', ' K', 'io', 's ', 'n ', 'it', 'in', ' o', 'ad', 'wh', 'al', 'us', 'ss', ' i', 'ne', ' c', ' v', 'd ', 'ou', ' r', 'er', 'wa', 'at', 'rc', 'nd', 'lt', 'a ', 'd,'}\n"
     ]
    }
   ],
   "source": [
    "k = 2\n",
    "a = shingle(a, k)\n",
    "b = shingle(b, k)\n",
    "c = shingle(c, k)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "b73ee0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(a.union(b).union(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "7462370e",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_1hot = [1 if x in a else 0 for x in vocab]\n",
    "b_1hot = [1 if x in b else 0 for x in vocab]\n",
    "c_1hot = [1 if x in c else 0 for x in vocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "981bbc6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154]\n"
     ]
    }
   ],
   "source": [
    "#Step 8-9\n",
    "hash_ex = list(range(1, len(vocab)+1))\n",
    "print(hash_ex)  # we haven't shuffled yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "65bb5cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[135, 66, 120, 9, 148, 152, 57, 108, 111, 14, 43, 67, 106, 144, 96, 154, 73, 28, 107, 93, 121, 97, 42, 113, 112, 143, 77, 92, 44, 55, 72, 87, 74, 60, 4, 3, 1, 139, 137, 17, 6, 24, 71, 80, 61, 15, 102, 86, 101, 35, 23, 142, 153, 45, 146, 13, 124, 76, 119, 53, 91, 56, 21, 70, 98, 52, 62, 12, 122, 54, 10, 123, 65, 90, 140, 78, 89, 85, 8, 103, 131, 18, 2, 7, 58, 141, 26, 34, 132, 95, 129, 99, 46, 5, 51, 150, 116, 110, 104, 63, 11, 25, 88, 117, 126, 48, 134, 118, 138, 16, 39, 49, 84, 105, 127, 27, 36, 100, 82, 128, 79, 109, 59, 29, 69, 115, 30, 22, 20, 136, 33, 31, 145, 81, 75, 130, 32, 41, 50, 125, 94, 47, 38, 37, 83, 19, 40, 149, 151, 64, 114, 133, 147, 68]\n"
     ]
    }
   ],
   "source": [
    "#Step 8-9\n",
    "from random import shuffle\n",
    "\n",
    "shuffle(hash_ex)\n",
    "print(hash_ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "52490c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 -> 83\n"
     ]
    }
   ],
   "source": [
    "print(f\"7 -> {hash_ex.index(7)}\") # note that value 7 can be found at index 1 in hash_ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "d90084c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 -> 36\n",
      "2 -> 82\n",
      "3 -> 35\n",
      "4 -> 34\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 5):\n",
    "    print(f\"{i} -> {hash_ex.index(i)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "ec6862e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 -> 36 -> 1\n",
      "match!\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, len(vocab)+1):\n",
    "    idx = hash_ex.index(i)\n",
    "    signature_val = a_1hot[idx]\n",
    "    print(f\"{i} -> {idx} -> {signature_val}\")\n",
    "    if signature_val == 1:\n",
    "        print('match!')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "03ed70c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#7-9\n",
    "def create_hash_func(size: int):\n",
    "    # function for creating the hash vector/function\n",
    "    hash_ex = list(range(1, len(vocab)+1))\n",
    "    shuffle(hash_ex)\n",
    "    return hash_ex\n",
    "\n",
    "def build_minhash_func(vocab_size: int, nbits: int):\n",
    "    # function for building multiple minhash vectors\n",
    "    hashes = []\n",
    "    for _ in range(nbits):\n",
    "        hashes.append(create_hash_func(vocab_size))\n",
    "    return hashes\n",
    "\n",
    "# we create 20 minhash vectors\n",
    "minhash_func = build_minhash_func(len(vocab), 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "6497b54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hash(vector: list):\n",
    "    # use this function for creating our signatures (eg the matching)\n",
    "    signature = []\n",
    "    for func in minhash_func:\n",
    "        for i in range(1, len(vocab)+1):\n",
    "            idx = func.index(i)\n",
    "            signature_val = vector[idx]\n",
    "            if signature_val == 1:\n",
    "                signature.append(i)\n",
    "                break\n",
    "    return signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "3f110d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 1, 1, 3, 2, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 1, 2, 1, 1, 1]\n",
      "[1, 3, 1, 1, 1, 3, 2, 1, 2, 1, 1, 2, 1, 1, 1, 2, 2, 1, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "# now create signatures\n",
    "a_sig = create_hash(a_1hot)\n",
    "b_sig = create_hash(b_1hot)\n",
    "c_sig = create_hash(c_1hot)\n",
    "\n",
    "print(a_sig)\n",
    "print(b_sig)\n",
    "#print(c_sig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a25e76",
   "metadata": {},
   "source": [
    "# Cosine Similarity using 3 ways \n",
    "https://danielcaraway.github.io/html/sklearn_cosine_similarity.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "2a3eb3ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.74921534 0.84714052]]\n",
      "[[1.         0.74921534 0.84714052]\n",
      " [0.74921534 1.         0.7675329 ]\n",
      " [0.84714052 0.7675329  1.        ]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# calculate cosine similarity between [X] and [Y,Z]\n",
    "# sending input as arrays would allow for calculating both cosine_sim(X,Y) and cosine_sim (X,Y)\n",
    "cos_sim = cosine_similarity([a_sig], [b_sig,c_sig])\n",
    "print(cos_sim)\n",
    "\n",
    "# calculate the entire cosie similarity matrix among X, Y, and Z\n",
    "cos_sim = cosine_similarity([a_sig, b_sig, c_sig])\n",
    "print(cos_sim)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "dea7af15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7675329008014159\n"
     ]
    }
   ],
   "source": [
    "from scipy import spatial\n",
    "\n",
    "cos_sim = 1 - spatial.distance.cosine(b_sig, c_sig)\n",
    "print(cos_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "cab9d384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8471405189362209\n"
     ]
    }
   ],
   "source": [
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "cos_sim = dot(a_sig,c_sig) / (norm(a_sig)*norm(c_sig))\n",
    "print(cos_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4dc634",
   "metadata": {},
   "source": [
    "# LSH itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "99669320",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_vector(signature, b):\n",
    "    assert len(signature) % b == 0\n",
    "    r = int(len(signature) / b)\n",
    "    # code splitting signature in b parts\n",
    "    subvecs = []\n",
    "    for i in range(0, len(signature), r):\n",
    "        subvecs.append(signature[i : i+r])\n",
    "    return subvecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "f3b9cb70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 3],\n",
       " [1, 1],\n",
       " [1, 3],\n",
       " [2, 1],\n",
       " [2, 1],\n",
       " [1, 2],\n",
       " [1, 1],\n",
       " [1, 2],\n",
       " [2, 1],\n",
       " [1, 2]]"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "band_a = split_vector(a_sig, 10)\n",
    "band_b = split_vector(b_sig, 10)\n",
    "band_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "450008be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3, 2],\n",
       " [2, 3],\n",
       " [2, 2],\n",
       " [1, 1],\n",
       " [2, 4],\n",
       " [3, 1],\n",
       " [1, 3],\n",
       " [1, 1],\n",
       " [1, 1],\n",
       " [2, 1]]"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "band_c = split_vector(c_sig, 10)\n",
    "band_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "1eb6253f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidate pair: [1, 2] == [1, 2]\n"
     ]
    }
   ],
   "source": [
    "for a_rows, b_rows in zip(band_a, band_b):\n",
    "    if a_rows == b_rows:\n",
    "        print(f\"Candidate pair: {a_rows} == {b_rows}\")\n",
    "        # we only need one band to match\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "e628f3e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2]]\n"
     ]
    }
   ],
   "source": [
    "print([a_rows])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "b6aabbfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidate pair: [1, 2] == [3, 1]\n"
     ]
    }
   ],
   "source": [
    "for a_rows, c_rows in zip(band_a, band_c):\n",
    "    if a_rows == b_rows:\n",
    "        print(f\"Candidate pair: {a_rows} == {c_rows}\")\n",
    "        # we only need one band to match\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "be23c6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for b_rows, c_rows in zip(band_b, band_c):\n",
    "    if a_rows == c_rows:\n",
    "        print(f\"Candidate pair: {b_rows} == {c_rows}\")\n",
    "        # we only need one band to match\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd836042",
   "metadata": {},
   "source": [
    "# Another Resource\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31f0bda0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: snapy in /Users/ibrahim/opt/anaconda3/lib/python3.9/site-packages (0.0.6)\n",
      "Requirement already satisfied: numpy>=1.13 in /Users/ibrahim/opt/anaconda3/lib/python3.9/site-packages (from snapy) (1.22.2)\n",
      "Requirement already satisfied: netCDF4>=1.5 in /Users/ibrahim/opt/anaconda3/lib/python3.9/site-packages (from snapy) (1.5.8)\n",
      "Requirement already satisfied: cftime in /Users/ibrahim/opt/anaconda3/lib/python3.9/site-packages (from netCDF4>=1.5->snapy) (1.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install snapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1dacae07",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'MinHash' from 'snapy' (/Users/ibrahim/opt/anaconda3/lib/python3.9/site-packages/snapy/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/82/58plxq9d79373wycjp789qfm0000gn/T/ipykernel_4814/3946355416.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0margparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msnapy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMinHash\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLSH\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mSEED\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'MinHash' from 'snapy' (/Users/ibrahim/opt/anaconda3/lib/python3.9/site-packages/snapy/__init__.py)"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    " \n",
    "import argparse\n",
    " \n",
    "from snapy import MinHash, LSH \n",
    "SEED = 3\n",
    " \n",
    " \n",
    "def load_content(sentence_file):\n",
    "    \"\"\"Load input file with sentences to build LSH.\n",
    " \n",
    "    Args:\n",
    "        sentence_file (str): Path to input with txt file with sentences to Build LSH.\n",
    " \n",
    "    Returns:\n",
    "        dict: Dict with strings and version of string in lower case and without comma.\n",
    " \n",
    "    \"\"\"\n",
    "    sentences = {}\n",
    "    with open(sentence_file) as content:\n",
    "        for line in content:\n",
    "            line = line.strip()\n",
    "            line_clean = line.replace(\",\", \"\")\n",
    "            line_clean = line_clean.lower()\n",
    "            sentences[line_clean] = line\n",
    " \n",
    "    return sentences\n",
    " \n",
    " \n",
    "def create_lsh(content, no_of_bands, n_permutations, n_gram):\n",
    "    \"\"\"Create Minhash and Locality Sensitive Hashing (LSH) to detect near duplicate texts.\n",
    " \n",
    "    Args:\n",
    "        content (list): List with string to build LSH.\n",
    "        no_of_bands (int): Number of bands to break minhash signature into before hashing into buckets.\n",
    "        n_permutations (int): Number of permutations used to create minhash signatures used in LSH model.\n",
    "        n_gram (int): Size of each overlapping text shingle to break text into prior to hashing.\n",
    "        no_of_bands(int): Number of bands to break minhash signature into before hashing into buckets.\n",
    " \n",
    "    Returns:\n",
    "        class 'snapy.lsh.LSH':  Snapy LSH object.\n",
    " \n",
    "    \"\"\"\n",
    "    labels = range(len(content))\n",
    " \n",
    "    # Create MinHash object.\n",
    "    minhash = MinHash(content, n_gram=n_gram, permutations=n_permutations, hash_bits=64, seed=SEED)\n",
    " \n",
    "    # Create LSH model.\n",
    "    lsh = LSH(minhash, labels, no_of_bands=no_of_bands)\n",
    " \n",
    "    return lsh\n",
    " \n",
    " \n",
    "def find_near_duplicate(query_sentences, sentences, min_jaccard_value, no_of_bands, n_permutations, n_gram):\n",
    "    \"\"\"Using LSH object finds the near duplicate strings.\n",
    " \n",
    "    Args:\n",
    "        query_sentences (dict): Dict with query strings and version of string in lower case and without comma.\n",
    "        sentences (dict): Dict with target strings and version of string in lower case and without comma.\n",
    "        min_jaccard_value (float): Minimum value for the Jaccard Distance.\n",
    "        no_of_bands (int): Number of bands to break minhash signature into before hashing into buckets.\n",
    "        n_permutations (int): Number of permutations used to create minhash signatures used in LSH model.\n",
    "        n_gram (int): Size of each overlapping text shingle to break text into prior to hashing.\n",
    " \n",
    "    \"\"\"\n",
    "    content = list(query_sentences.keys()) + list(sentences.keys())\n",
    "    lsh = create_lsh(content, no_of_bands, n_permutations, n_gram)\n",
    " \n",
    "    # Query to find near duplicates the string in `search`\n",
    "    closest_results = lsh.query(0, min_jaccard=min_jaccard_value)\n",
    " \n",
    "    for index_query, search_string in enumerate(query_sentences):\n",
    "        print(\"{} QUERY: {}\".format(index_query + 1, query_sentences[search_string]))\n",
    "        for content_index in closest_results:\n",
    "            result = content[content_index]\n",
    "            print(sentences[result])\n",
    "        print()\n",
    " \n",
    " \n",
    "def parse_args():\n",
    "    \"\"\"Parse args entered by the user.\n",
    " \n",
    "    Returns:\n",
    "        argparse.Namespace: Parsed arguments.\n",
    " \n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Detect near duplicate texts using Minhash and Locality Sensitive Hashing.\",\n",
    "        epilog=\"example > python3 find_near_duplicate.py  -q INPUT -t TARGERS\")\n",
    "    parser.add_argument(\"-q\", \"--query\", help=\"Path to file with sentences to query\", required=True)\n",
    "    parser.add_argument(\"-t\", \"--targets\", help=\"Path to file with sentences be matched against\", required=True)\n",
    "    parser.add_argument(\"-g\", \"--n_gram\", help=\"Size of each overlapping text shingle to break text into \"\n",
    "                                               \"prior to hashing\", default=9)\n",
    "    parser.add_argument(\"-p\", \"--n_permutations\", help=\"Number of permutations used to create minhash signatures used \"\n",
    "                                                       \"in LSH model.\", default=100)\n",
    "    parser.add_argument(\"-j\", \"--min_jaccard\", help=\"Jaccard similarity threshold texts have to exceed to be \"\n",
    "                                                          \"returned as similar.\", default=0.25)\n",
    "    parser.add_argument(\"-b\", \"--no_of_bands\", help=\"Number of bands to break minhash signature into \"\n",
    "                                                    \"before hashing into buckets..\", default=50)\n",
    "    return parser.parse_args()\n",
    " \n",
    " \n",
    "def main():\n",
    "    args = parse_args()\n",
    " \n",
    "    query = args.query\n",
    "    targets = args.targets\n",
    "    min_jaccard_value = float(args.min_jaccard)\n",
    "    n_gram = int(args.n_gram)\n",
    "    n_permutations = int(args.n_permutations)\n",
    "    no_of_bands = int(args.no_of_bands)\n",
    " \n",
    "    # load sentences from file\n",
    "    query_sentences = load_content(query)\n",
    "    targets_sentences = load_content(targets)\n",
    " \n",
    "    # find near duplicate sequences to `search_string`\n",
    "    find_near_duplicate(query_sentences, targets_sentences, min_jaccard_value, no_of_bands, n_permutations, n_gram)\n",
    " \n",
    " \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9142270",
   "metadata": {},
   "source": [
    "# Test NW with Lemmatization+ POS+ Synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2b817c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "3e558b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score -2\n",
      "Identity 25.0\n",
      "Alignment of Query and Target Sentence is:\n",
      "\therewe-are!\n",
      "\t-wearehere!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from minineedle import needle, core\n",
    "\n",
    "seq1='Here we are!'.lower().split()\n",
    "seq2='We are here!'.lower().split()\n",
    "\n",
    "alignment = needle.NeedlemanWunsch(seq1, seq2)\n",
    "\n",
    "alignment.align()\n",
    "\n",
    "print('Score',alignment.get_score())\n",
    "\n",
    "print('Identity',alignment.get_identity())\n",
    "\n",
    "print(alignment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "76c4a64d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score 10\n",
      "Identity 75.0\n",
      "Alignment of SEQUENCE 1 and SEQUENCE 2:\n",
      "\tthis is a good night\n",
      "\tthis is a good --day\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from minineedle import needle, core\n",
    "\n",
    "seq1='This is a good night'.lower()\n",
    "seq2='This is a good day'.lower()\n",
    "\n",
    "alignment = needle.NeedlemanWunsch(seq1, seq2)\n",
    "\n",
    "alignment.align()\n",
    "\n",
    "print('Score',alignment.get_score())\n",
    "\n",
    "print('Identity',alignment.get_identity())\n",
    "\n",
    "print(alignment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1ed8b1df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>day</th>\n",
       "      <th>good</th>\n",
       "      <th>is</th>\n",
       "      <th>night</th>\n",
       "      <th>this</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>seq1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>seq2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      day  good  is  night  this\n",
       "seq1    0     1   1      1     1\n",
       "seq2    1     1   1      0     1"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scikit Learn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "documents=[seq1,seq2]\n",
    "# Create the Document Term Matrix\n",
    "count_vectorizer = CountVectorizer(stop_words='english')\n",
    "count_vectorizer = CountVectorizer()\n",
    "sparse_matrix = count_vectorizer.fit_transform(documents)\n",
    "\n",
    "# OPTIONAL: Convert Sparse Matrix to Pandas Dataframe if you want to see the word frequencies.\n",
    "doc_term_matrix = sparse_matrix.todense()\n",
    "df = pd.DataFrame(doc_term_matrix, \n",
    "                  columns=count_vectorizer.get_feature_names(), \n",
    "                  index=['seq1', 'seq2'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a11e296f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.   0.75]\n",
      " [0.75 1.  ]]\n"
     ]
    }
   ],
   "source": [
    "# Compute Cosine Similarity\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "print(cosine_similarity(df, df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4096260b",
   "metadata": {},
   "source": [
    "# Get Synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5ba883d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'necropolis', 'graveyard', 'burial_site', 'cemetery', 'burial_ground', 'memorial_park', 'burying_ground'}\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "synonyms = []\n",
    "antonyms = []\n",
    "  \n",
    "for syn in wordnet.synsets(\"cemetery\"):\n",
    "    for l in syn.lemmas():\n",
    "        synonyms.append(l.name())\n",
    "        if l.antonyms():\n",
    "            antonyms.append(l.antonyms()[0].name())\n",
    "  \n",
    "print(set(synonyms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b11dfef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b571d6c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bcc4e356",
   "metadata": {},
   "source": [
    "# Test for Lemmatization and POS\n",
    "### https://www.machinelearningplus.com/nlp/lemmatization-examples-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1654798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ask\n",
      "I saw Susie sit in a shoe shine shop . Where Susie sits Susie shine , and where Susie shine Susie sits\n"
     ]
    }
   ],
   "source": [
    "# Lemmatize with POS Tag\n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "\n",
    "# 1. Init Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# 2. Lemmatize Single Word with the appropriate POS tag\n",
    "word = 'asked'\n",
    "print(lemmatizer.lemmatize(word, get_wordnet_pos(word)))\n",
    "\n",
    "# 3. Lemmatize a Sentence with the appropriate POS tag\n",
    "sentence = \"I saw Susie sitting in a shoe shine shop. Where Susie sits Susie shines, and where Susie shines Susie sits\"\n",
    "\n",
    "sentence2 = ' '.join([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(sentence)]) \n",
    "print(sentence2)\n",
    "#> ['The', 'strip', 'bat', 'be', 'hang', 'on', 'their', 'foot', 'for', 'best']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2045a2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def are_synonyms(x,y):# To check for synonyms between seq1 and seq2 return (True or False) \n",
    "             synonyms2 = []\n",
    "             for syn in wordnet.synsets(y):\n",
    "                 for lm in syn.lemmas():\n",
    "                       synonyms2.append(lm.name())\n",
    "             synonyms=set(synonyms2)\n",
    "             if x in synonyms:\n",
    "                  return True\n",
    "             else:\n",
    "                  return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "06635106",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "are_synonyms('was','was')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60d87689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ask\n",
      "ask\n"
     ]
    }
   ],
   "source": [
    "# Lemmatize with POS Tag\n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    \n",
    "\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "\n",
    "\n",
    "# 1. Init Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# 2. Lemmatize Single Word with the appropriate POS tag\n",
    "\n",
    "\n",
    "x = lemmatizer.lemmatize(word, get_wordnet_pos(word))\n",
    "y = lemmatizer.lemmatize(word, get_wordnet_pos(word))\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad6cb555",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "        \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "        tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    \n",
    "\n",
    "        tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "        return tag_dict.get(tag, wordnet.NOUN)\n",
    "    \n",
    "def checklemma_pos(word1,word2):\n",
    "\n",
    "# 1. Init Lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    \n",
    "    x = lemmatizer.lemmatize(word1, get_wordnet_pos(word1))\n",
    "    y = lemmatizer.lemmatize(word2, get_wordnet_pos(word2))\n",
    "    if x == y:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49e2d29b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checklemma_pos('dead','die')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aa5d0866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "where\n",
      "['I', 'saw', 'Susie', 'sit', 'in', 'a', 'shoe', 'shine', 'shop', '.', 'Where', 'Susie', 'sits', 'Susie', 'shine', ',', 'and', 'where', 'Susie', 'shine', 'Susie', 'sits']\n"
     ]
    }
   ],
   "source": [
    "# Lemmatize with POS Tag\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "\n",
    "# 1. Init Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# 2. Lemmatize Single Word with the appropriate POS tag\n",
    "word = 'where'\n",
    "print(lemmatizer.lemmatize(word, get_wordnet_pos(word)))\n",
    "\n",
    "# 3. Lemmatize a Sentence with the appropriate POS tag\n",
    "sentence = \"I saw Susie sitting in a shoe shine shop. Where Susie sits Susie shines, and where Susie shines Susie sits\"\n",
    "print([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(sentence)])\n",
    "#> ['The', 'strip', 'bat', 'be', 'hang', 'on', 'their', 'foot', 'for', 'best']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5be3c6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DT'), ('rest', 'NN'), ('of', 'IN'), ('life', 'NN')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import pos_tag, word_tokenize\n",
    "pos_tag(word_tokenize(\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bdd5c701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('There', 'EX'), ('is', 'VBZ'), ('a', 'DT'), ('stubbornness', 'NN'), ('about', 'IN'), ('me', 'PRP'), ('that', 'IN'), ('never', 'RB'), ('can', 'MD'), ('bear', 'VB'), ('to', 'TO'), ('be', 'VB'), ('frightened', 'VBN'), ('at', 'IN'), ('the', 'DT'), ('will', 'MD'), ('of', 'IN'), ('others', 'NNS'), ('.', '.'), ('My', 'PRP$'), ('courage', 'NN'), ('always', 'RB'), ('rises', 'VBZ'), ('at', 'IN'), ('every', 'DT'), ('attempt', 'NN'), ('to', 'TO'), ('intimidate', 'VB'), ('me', 'PRP'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "sentence = \"There is a stubbornness about me that never can bear to be frightened at the will of others. My courage always rises at every attempt to intimidate me.\"\n",
    "print (nltk.pos_tag(word_tokenize(sentence)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29605f2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
